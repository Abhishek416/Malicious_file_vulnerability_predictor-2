import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statistics import mean

df = pd.read_csv('result.csv')

import argparse
import requests
import time

parser = argparse.ArgumentParser(description='Query sample information by Hash or File.')

parser.add_argument('--file', dest='file', type=str, help='Query File at filepath (e.g. /foo/bar/blah.exe)')
parser.add_argument('--hash', dest='hash', type=str, help='Query Hash (MD5)')
args = parser.parse_args()

def malbazaarlookup(df):
    for x in range(0, df.shape[0]//100+1):
        for ind in range(x*100, min(df.shape[0], (x+1)*100)):
            print("Remaining to be updated: ", df.shape[0]-ind)
            data = {'query': 'get_info', 'hash': df['md5'][ind]}
            url = "https://mb-api.abuse.ch/api/v1/"
            response = requests.post(url, data=data)
            
            if response.json()["query_status"] == 'hash_not_found':
                print('>>>>>>>>>>  The sample hash was not found on Malware bazaar  <<<<<<<<<<')
            elif response.json()["data"] == None:
                print(">>>>>>>>>> Sample hash has no data in Malware bazaar <<<<<<<<<<")
            else:
                response_json = response.json()["data"][0]
                
                file_name = response_json.get("file_name")
                file_size = response_json.get("file_size")
                signature = response_json.get("signature")
                delivery_method = response_json.get("delivery_method")
                
                file_type_mime = response_json.get("file_type_mime")
                file_type = response_json.get("file_type")
                
                df.loc[ind, "file_name"] = file_name
                df.loc[ind, "file_size"] = file_size
                df.loc[ind, "signature"] = signature
                df.loc[ind, "delivery_method"] = delivery_method
                
                df.loc[ind, "file_type_mime"] = file_type_mime
                df.loc[ind, "file_type"] = file_type
        
        time.sleep(5)

    return df


# df = malbazaarlookup(df)
    
df.head()
print(df.columns)
print(df.isnull().sum())

# df.to_csv('result_updated.csv')

# df = pd.read_csv('result_updated.csv')

columns = df.columns
min_null = 10

# print(df.dtypes)

for col in columns:
    if df[col].isnull().sum() > min_null:
        df = df.drop([col], axis=1)

# missing values
df = df.replace(np.nan, False)
# print("Final columns (without null values):", df.columns)
# print("Maximum null values left: ", max(df.isnull().sum()))

df = df.replace({True: 'TRUE', False: 'FALSE'})

"""
print(len(df["file_name"].unique()))
print(len(df["file_size"].unique()))
print(len(df["signature"].unique()))
print(len(df["delivery_method"].unique()))

print(df["file_type_mime"].unique())
print(df["file_type"].unique())

X = df.iloc[:, [4, 5, 6, -8, -7, -6, -5, -4, -3, -2, -1]].values
y = df.iloc[:, 9:].values
"""

df['scan_date'] = pd.to_datetime(df['scan_date'], format='%Y-%m-%d %H:%M:%S')

for sha256 in df['sha256']:
    for i in range(0, 64, 2):
        df['s'+str(i//2)] = int(sha256[i:i+2], base=16)
        
df['day'] = df['scan_date'].dt.day
df['month'] = df['scan_date'].dt.month
df['year'] = df['scan_date'].dt.year

X = df.iloc[:, [5, 6, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1]].values
y = df.iloc[:, 9:-32].values


#PCA
from sklearn.decomposition import PCA

pca = PCA(n_components=5)
X = pca.fit_transform(X)

# Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer
"""
labelencoder_X = LabelEncoder()
X[:, 5] = labelencoder_X.fit_transform(X[:, 5])
X[:, 6] = labelencoder_X.fit_transform(X[:, 6])
X[:, 9] = labelencoder_X.fit_transform(X[:, 9])
X[:, 10] = labelencoder_X.fit_transform(X[:, 10])
ct = ColumnTransformer([("Country", OneHotEncoder(), [5, 6, 9, 10])], remainder='passthrough')
X = ct.fit_transform(X)
"""
ord_enc = OrdinalEncoder()
y = ord_enc.fit_transform(y)

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Feature scaling
from sklearn.preprocessing import StandardScaler

sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

# Classification model
# Fitting Decision tree classifier to the Training set
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

print("Decision Tree:")

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay

cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))
print(cm)

acc= []
for i in range(y_pred.shape[0]):
    acc.append(accuracy_score(y_test[i], y_pred[i]))
print('Accuracy: %.3f' % mean(acc))

from sklearn.metrics import r2_score, fbeta_score, mean_absolute_error, mean_squared_error
r2 = r2_score(y_test, y_pred)
f2 = fbeta_score(y_test, y_pred, beta=2, average='macro')
abs_error = mean_absolute_error(y_test, y_pred)
squ_error = mean_squared_error(y_test, y_pred)

print("R2 score: ", r2, "\nF2 score: ", f2, "\nMean Absolute Error: ", abs_error, "\nMean Square Error: ", squ_error)

# Fitting Random Forest classifier to the Training set
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0)
rf.fit(X_train, y_train)

# Predicting the Test set results
y_pred = rf.predict(X_test)

print("Random Forest:")

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score

cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))
print(cm)

acc= []
for i in range(y_pred.shape[0]):
    acc.append(accuracy_score(y_test[i], y_pred[i]))
print('Accuracy: %.3f' % mean(acc))

from sklearn.metrics import r2_score, fbeta_score, mean_absolute_error, mean_squared_error
r2 = r2_score(y_test, y_pred)
f2 = fbeta_score(y_test, y_pred, beta=2, average='macro')
abs_error = mean_absolute_error(y_test, y_pred)
squ_error = mean_squared_error(y_test, y_pred)

print("R2 score: ", r2, "\nF2 score: ", f2, "\nMean Absolute Error: ", abs_error, "\nMean Square Error: ", squ_error)

# Fitting KNN classifier to the Training set
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=10, metric='minkowski', p=2)
knn.fit(X_train, y_train)

# Predicting the Test set results
y_pred = knn.predict(X_test)

print('KNN classifier: ')

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score

cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))
print(cm)

acc= []
for i in range(y_pred.shape[0]):
    acc.append(accuracy_score(y_test[i], y_pred[i]))
print('Accuracy: %.3f' % mean(acc))

from sklearn.metrics import r2_score, fbeta_score, mean_absolute_error, mean_squared_error
r2 = r2_score(y_test, y_pred)
f2 = fbeta_score(y_test, y_pred, beta=2, average='macro')
abs_error = mean_absolute_error(y_test, y_pred)
squ_error = mean_squared_error(y_test, y_pred)

print("R2 score: ", r2, "\nF2 score: ", f2, "\nMean Absolute Error: ", abs_error, "\nMean Square Error: ", squ_error)

# Training the XGB Classifier model on the Training set
from xgboost import XGBClassifier

xgb = XGBClassifier(n_estimators = 1000, learning_rate = 0.1, max_depth = 3)
xgb.fit(X_train, y_train)

# Predicting the Test set results
y_pred = xgb.predict(X_test)

print('XGB classifier: ')

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score

cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))
print(cm)

acc= []
for i in range(y_pred.shape[0]):
    acc.append(accuracy_score(y_test[i], y_pred[i]))
print('Accuracy: %.3f' % mean(acc))

from sklearn.metrics import r2_score, fbeta_score, mean_absolute_error, mean_squared_error
r2 = r2_score(y_test, y_pred)
f2 = fbeta_score(y_test, y_pred, beta=2, average='macro')
abs_error = mean_absolute_error(y_test, y_pred)
squ_error = mean_squared_error(y_test, y_pred)

print("R2 score: ", r2, "\nF2 score: ", f2, "\nMean Absolute Error: ", abs_error, "\nMean Square Error: ", squ_error)

"""
import pickle

filename = 'model.h5'
pickle.dump(rf, open(filename, 'wb'))

pickle.dump(sc_X, open('scalarX.h5', 'wb'))

pickle.dump(pca, open('pca.h5', 'wb'))
"""

# file_path = 'ML cheat sheet.jpg'

# from py_essentials import hashing as hs
# sha256 = hs.fileChecksum(file_path, "sha256")

# print(sha256)

# data = []
# for i in range(0, 64, 2):
#     data.append(int(sha256[i:i+2], base=16))
        
# data = sc_X.transform([data])

# output = []

# output.append(classifier.predict(data)[0])
# output.append(rf.predict(data)[0])
# output.append(knn.predict(data)[0])
# output.append(xgb.predict(data)[0])

# """
# print("Decision tree: ", classifier.predict(data))
# print("Random Forest: ", rf.predict(data))
# print("KNN: ", knn.predict(data))
# print("XgBoost: ", xgb.predict(data))
# """

# result = pd.DataFrame(output, columns=df.columns[9:-32], index=['Decision Tree', 'Random Forest', 'KNN', 'XgBoost'])

# result.to_csv('output.csv')
